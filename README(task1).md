In this project, several crucial data cleaning steps were implemented to ensure the dataset is accurate, consistent, and ready for analysis. First, missing values were identified using the .isnull() method, and handling techniques were applied to make sure that no missing data would interfere with further analysis. Duplicate rows were identified and removed using .drop_duplicates(), ensuring the dataset remained unique and free of redundancy. Text values such as make,model,trim and body names were standardized by applying consistent formats and capitalized methods. Additionally, date formats were standardized from 'Tue Dec 16 2014 12:30:00 GMT-0800 (PST)' to a consistent 'dd-mm-yyyy' format using pd.to_datetime(), making it easier to work with temporal data. Column headers were renamed to ensure uniformity, converting them to lowercase and removing spaces for a cleaner, more accessible structure. Finally, data types were checked and corrected: columns like odometer  were converted to float64, and dates were converted to proper datetime objects, ensuring that all fields were appropriately typed for analysis. These cleaning operations were crucial for transforming the raw dataset into a well-organized and reliable resource, ensuring its readiness for further exploration and modeling.
